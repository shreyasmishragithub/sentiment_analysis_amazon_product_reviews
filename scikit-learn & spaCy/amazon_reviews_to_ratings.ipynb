{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PANDAS, REGULAR EXPRESSION and NUMPY\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAmazon_Datasets \u001b[m\u001b[m/                  amazon_reviews_to_Sentiment.ipynb\r\n",
      "DASK.ipynb                         amazon_reviews_to_ratings.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets \n"
     ]
    }
   ],
   "source": [
    "cd '/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets /'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>25933450</td>\n",
       "      <td>RJOVP071AVAJO</td>\n",
       "      <td>0439873800</td>\n",
       "      <td>84656342</td>\n",
       "      <td>There Was an Old Lady Who Swallowed a Shell!</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>I love it and so does my students!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1801372</td>\n",
       "      <td>R1ORGBETCDW3AI</td>\n",
       "      <td>1623953553</td>\n",
       "      <td>729938122</td>\n",
       "      <td>I Saw a Friend</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Please buy \"I Saw a Friend\"! Your children wil...</td>\n",
       "      <td>My wife and I ordered 2 books and gave them as...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>5782091</td>\n",
       "      <td>R7TNRFQAOUTX5</td>\n",
       "      <td>142151981X</td>\n",
       "      <td>678139048</td>\n",
       "      <td>Black Lagoon, Vol. 6</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Shipped fast.</td>\n",
       "      <td>Great book just like all the others in the ser...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>32715830</td>\n",
       "      <td>R2GANXKDIFZ6OI</td>\n",
       "      <td>014241543X</td>\n",
       "      <td>712432151</td>\n",
       "      <td>If I Stay</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>So beautiful</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>14005703</td>\n",
       "      <td>R2NYB6C3R8LVN6</td>\n",
       "      <td>1604600527</td>\n",
       "      <td>800572372</td>\n",
       "      <td>Stars 'N Strips Forever</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Enjoyed the author's story and his quilts are ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     25933450   RJOVP071AVAJO  0439873800        84656342   \n",
       "1          US      1801372  R1ORGBETCDW3AI  1623953553       729938122   \n",
       "2          US      5782091   R7TNRFQAOUTX5  142151981X       678139048   \n",
       "3          US     32715830  R2GANXKDIFZ6OI  014241543X       712432151   \n",
       "4          US     14005703  R2NYB6C3R8LVN6  1604600527       800572372   \n",
       "\n",
       "                                  product_title product_category star_rating  \\\n",
       "0  There Was an Old Lady Who Swallowed a Shell!            Books           5   \n",
       "1                                I Saw a Friend            Books           5   \n",
       "2                          Black Lagoon, Vol. 6            Books           5   \n",
       "3                                     If I Stay            Books           5   \n",
       "4                       Stars 'N Strips Forever            Books           5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            0.0          0.0    N                 Y   \n",
       "1            0.0          0.0    N                 Y   \n",
       "2            0.0          0.0    N                 Y   \n",
       "3            0.0          0.0    N                 N   \n",
       "4            2.0          2.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Please buy \"I Saw a Friend\"! Your children wil...   \n",
       "2                                      Shipped fast.   \n",
       "3                                         Five Stars   \n",
       "4                                         Five Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                 I love it and so does my students!  2015-08-31  \n",
       "1  My wife and I ordered 2 books and gave them as...  2015-08-31  \n",
       "2  Great book just like all the others in the ser...  2015-08-31  \n",
       "3                                       So beautiful  2015-08-31  \n",
       "4  Enjoyed the author's story and his quilts are ...  2015-08-31  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#books = pd.read_table('/Users/siddharthmandgi/Desktop/Amazon_Reviews_to_Rating/Amazon_Datasets /amazon_reviews_us_Books_v1_00.tsv',error_bad_lines=False)\n",
    "books.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10236850 entries, 0 to 10236849\n",
      "Data columns (total 15 columns):\n",
      "marketplace          object\n",
      "customer_id          int64\n",
      "review_id            object\n",
      "product_id           object\n",
      "product_parent       int64\n",
      "product_title        object\n",
      "product_category     object\n",
      "star_rating          object\n",
      "helpful_votes        float64\n",
      "total_votes          float64\n",
      "vine                 object\n",
      "verified_purchase    object\n",
      "review_headline      object\n",
      "review_body          object\n",
      "review_date          object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace             0\n",
       "customer_id             0\n",
       "review_id               0\n",
       "product_id              0\n",
       "product_parent          0\n",
       "product_title           0\n",
       "product_category        0\n",
       "star_rating            60\n",
       "helpful_votes          93\n",
       "total_votes            93\n",
       "vine                   93\n",
       "verified_purchase      93\n",
       "review_headline       163\n",
       "review_body           287\n",
       "review_date          1128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10235459, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '2', '4', '3', '1', 5, 1, 3, 4, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = books['star_rating'].unique().tolist() #non uniform datatype\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "books['star_rating'] = books['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 4, 3, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = books['star_rating'].unique().tolist() #uniform datatype\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA With spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [word.lemma_.lower() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    mytokens = [word for word in mytokens if word not in stopwords and word not in punctuations]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self,deep=True):\n",
    "        return{}\n",
    "    \n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdifVect = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down a review into dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3fc65f79431c455caaf093c26b691a6b-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Quality</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">product</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">fast</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">shipping.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3fc65f79431c455caaf093c26b691a6b-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3fc65f79431c455caaf093c26b691a6b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3fc65f79431c455caaf093c26b691a6b-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3fc65f79431c455caaf093c26b691a6b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3fc65f79431c455caaf093c26b691a6b-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3fc65f79431c455caaf093c26b691a6b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(books['review_body'][9]) \n",
    "displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = books.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.drop('star_rating',axis=1)\n",
    "X = data['review_body']\n",
    "y = data['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75232    My daughter loves these books!  We buy them al...\n",
       "48963    Jeff Shavitz’s book, “Size Doesn’t Matter: Why...\n",
       "44972                    old book, old info, in bad shape.\n",
       "13572                                            very good\n",
       "92740                                            thank you\n",
       "                               ...                        \n",
       "6267                                    abc in a cute way.\n",
       "54895    Cute idea of a story but the way the sentences...\n",
       "76832    Susan May Warren in her new book “Always on My...\n",
       "860      Wonderful, uplifting books for self and for gi...\n",
       "15800    Excellent thesis on Senator Warren's ideas and...\n",
       "Name: review_body, Length: 80000, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75232    5\n",
       "48963    5\n",
       "44972    2\n",
       "13572    5\n",
       "92740    5\n",
       "        ..\n",
       "6267     4\n",
       "54895    2\n",
       "76832    5\n",
       "860      5\n",
       "15800    5\n",
       "Name: star_rating, Length: 80000, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x137b2fe50>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 )\n",
      "Here I am.  Now I am here.  I am.  Yes.  Enjoy being. 'PREDICTION': 5\n",
      "\n",
      " 2 )\n",
      "Great book, simply love it! It was a pleasure coloring, made me feel so much more relaxed. I highly recommend it and can't wait for the next one to be published. 'PREDICTION': 5\n",
      "\n",
      " 3 )\n",
      "Phenomenal read for anyone in transition! This book daily takes you through spiritual disciplines that help you stay focused on the fullest life in Christ and how to handle the post grad experience.  So thankful for Mrs. Robin and her words of encouragement.  Would recommend to the closest of friends! 'PREDICTION': 5\n",
      "\n",
      " 4 )\n",
      "very abstract... did nothing for me... waste of money do not buy!!!! no trading advise what so ever! 'PREDICTION': 1\n",
      "\n",
      " 5 )\n",
      "My review is strictly for The Hobbit & The Lord of the Rings Deluxe Pocket set.<br /><br />I love this set of books. Before I purchased, I took some time to read reviews on Amazon. Most reviewers mentioned that the font size was small. I have no issue with being able to read Tolkien's masterpiece. But, that is just me. As for quality, it is evident. The leather is soft and as leather should be. The box set comes in an embossed leather holding case which keeps the small books together. Other reviewers have mentioned that the colors of the covers are dull. I personally think that they are beyond beautiful. The colors are perfect for a set like this, from The Hobbit's purple syrah color to the copper of The Two Towers, to the army green color of The Return of the King. Another point that made me purchase this set over others was the size. You are able to carry these otherwise massive books with you anywhere. Believe me, I've been meaning to reread this series but purchased a huge edition with all three stories, sans The Hobbit, in one bindup. After getting past twelve pages I just couldn't do it anymore. This, however, is a set that is well worth your money. 'PREDICTION': 5\n",
      "\n",
      " 6 )\n",
      "Book is even better than the movie! 'PREDICTION': 5\n",
      "\n",
      " 7 )\n",
      "As a Notre Dame fan I really enjoyed  the perspective Mr. Pomarico provides of his time in South Bend. This book is more than just about Notre Dame football. It really is full of life lessons and I would encourage anyone who enjoys studying how others achieve success to read this. The motivational ideas inside the book can be applied to all areas of life. 'PREDICTION': 5\n",
      "\n",
      " 8 )\n",
      "its a must in your office for your daily work. 'PREDICTION': 5\n",
      "\n",
      " 9 )\n",
      "It is a good guide for people new to dating. 'PREDICTION': 5\n",
      "\n",
      " 10 )\n",
      "Having been an avid reader all my life, this book is probably the best I have ever read in this genre but also the most scary. It covers a comprehensive re-evaluation of US history over the last 150 years. The scary part is that a very different picture of the US as a world power emerges. The assessments offered are powerful and credible as every statement is backed up by references to declassified documents, interviews or correspondence. This book is a must read if there ever was one! 'PREDICTION': 5\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for (sample,pred) in zip(X_test[0:10],sample_prediction[0:10]):\n",
    "    print('\\n',count,')')\n",
    "    print(sample,\"'PREDICTION':\", pred)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74745\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy\n",
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"I love this book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Waste of Money!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict([\"Fake! got stones instead!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =  LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12f20ea90>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy\n",
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12d2d4a10>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x11da0a4d0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68555\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(n_estimators=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x11edfa210>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x127145cb0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "                                   bootstrap_features=False, max_features=1.0,\n",
       "                                   max_samples=1.0, n_estimators=10,\n",
       "                                   n_jobs=None, oob_score=False, random_state=7,\n",
       "                                   verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x11ef70bd0>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x127145cb0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "                                    learning_rate=1.0, n_estimators=100,\n",
       "                                    random_state=7))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                ('tfidfVect', tfdifVect),\n",
    "                ('classifier',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x12f50b710>),\n",
       "                ('tfidfVect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=N...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=42,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74085\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batch-Wise Predictions for our Big Data (1+ GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **LOGISTIC REGRESSION** for ensembling since it provided the highest accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_list = np.array_split(books, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>25933450</td>\n",
       "      <td>RJOVP071AVAJO</td>\n",
       "      <td>0439873800</td>\n",
       "      <td>84656342</td>\n",
       "      <td>There Was an Old Lady Who Swallowed a Shell!</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>I love it and so does my students!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1801372</td>\n",
       "      <td>R1ORGBETCDW3AI</td>\n",
       "      <td>1623953553</td>\n",
       "      <td>729938122</td>\n",
       "      <td>I Saw a Friend</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Please buy \"I Saw a Friend\"! Your children wil...</td>\n",
       "      <td>My wife and I ordered 2 books and gave them as...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>5782091</td>\n",
       "      <td>R7TNRFQAOUTX5</td>\n",
       "      <td>142151981X</td>\n",
       "      <td>678139048</td>\n",
       "      <td>Black Lagoon, Vol. 6</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Shipped fast.</td>\n",
       "      <td>Great book just like all the others in the ser...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>32715830</td>\n",
       "      <td>R2GANXKDIFZ6OI</td>\n",
       "      <td>014241543X</td>\n",
       "      <td>712432151</td>\n",
       "      <td>If I Stay</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>So beautiful</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>14005703</td>\n",
       "      <td>R2NYB6C3R8LVN6</td>\n",
       "      <td>1604600527</td>\n",
       "      <td>800572372</td>\n",
       "      <td>Stars 'N Strips Forever</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Enjoyed the author's story and his quilts are ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047383</td>\n",
       "      <td>US</td>\n",
       "      <td>35585422</td>\n",
       "      <td>R39HG02Y8V7LWE</td>\n",
       "      <td>0310435307</td>\n",
       "      <td>260370434</td>\n",
       "      <td>NIV Giant Print Compact Bible</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Excellent.</td>\n",
       "      <td>Perfect print size and easy to handle. Too bad...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047384</td>\n",
       "      <td>US</td>\n",
       "      <td>8699723</td>\n",
       "      <td>R2X6DNUCOZV015</td>\n",
       "      <td>0979278031</td>\n",
       "      <td>313033861</td>\n",
       "      <td>Jetty Man</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great series of books based in our area</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047385</td>\n",
       "      <td>US</td>\n",
       "      <td>48359513</td>\n",
       "      <td>R3PZ4X28BR5289</td>\n",
       "      <td>1596435828</td>\n",
       "      <td>513092252</td>\n",
       "      <td>Giants Beware! (The Chronicles of Claudette)</td>\n",
       "      <td>Books</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Amazing book- great story with incredible illu...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047386</td>\n",
       "      <td>US</td>\n",
       "      <td>7032112</td>\n",
       "      <td>R198Y7OVTU9IQ4</td>\n",
       "      <td>0764143573</td>\n",
       "      <td>455553794</td>\n",
       "      <td>Barron's Law Dictionary: Mass Market Edition (...</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2047387</td>\n",
       "      <td>US</td>\n",
       "      <td>52554709</td>\n",
       "      <td>R2NPJ1FTZ0E8RB</td>\n",
       "      <td>1416553649</td>\n",
       "      <td>335505464</td>\n",
       "      <td>Born Standing Up: A Comic's Life</td>\n",
       "      <td>Books</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Humorous and Insightful</td>\n",
       "      <td>It's fun read with both humor and insights int...</td>\n",
       "      <td>2015-02-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2047092 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     25933450   RJOVP071AVAJO  0439873800        84656342   \n",
       "1                US      1801372  R1ORGBETCDW3AI  1623953553       729938122   \n",
       "2                US      5782091   R7TNRFQAOUTX5  142151981X       678139048   \n",
       "3                US     32715830  R2GANXKDIFZ6OI  014241543X       712432151   \n",
       "4                US     14005703  R2NYB6C3R8LVN6  1604600527       800572372   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2047383          US     35585422  R39HG02Y8V7LWE  0310435307       260370434   \n",
       "2047384          US      8699723  R2X6DNUCOZV015  0979278031       313033861   \n",
       "2047385          US     48359513  R3PZ4X28BR5289  1596435828       513092252   \n",
       "2047386          US      7032112  R198Y7OVTU9IQ4  0764143573       455553794   \n",
       "2047387          US     52554709  R2NPJ1FTZ0E8RB  1416553649       335505464   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0             There Was an Old Lady Who Swallowed a Shell!            Books   \n",
       "1                                           I Saw a Friend            Books   \n",
       "2                                     Black Lagoon, Vol. 6            Books   \n",
       "3                                                If I Stay            Books   \n",
       "4                                  Stars 'N Strips Forever            Books   \n",
       "...                                                    ...              ...   \n",
       "2047383                      NIV Giant Print Compact Bible            Books   \n",
       "2047384                                          Jetty Man            Books   \n",
       "2047385       Giants Beware! (The Chronicles of Claudette)            Books   \n",
       "2047386  Barron's Law Dictionary: Mass Market Edition (...            Books   \n",
       "2047387                   Born Standing Up: A Comic's Life            Books   \n",
       "\n",
       "         star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                  5            0.0          0.0    N                 Y   \n",
       "1                  5            0.0          0.0    N                 Y   \n",
       "2                  5            0.0          0.0    N                 Y   \n",
       "3                  5            0.0          0.0    N                 N   \n",
       "4                  5            2.0          2.0    N                 Y   \n",
       "...              ...            ...          ...  ...               ...   \n",
       "2047383            5            0.0          1.0    N                 Y   \n",
       "2047384            5            0.0          0.0    N                 Y   \n",
       "2047385            4            1.0          1.0    N                 N   \n",
       "2047386            5            0.0          0.0    N                 Y   \n",
       "2047387            5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Please buy \"I Saw a Friend\"! Your children wil...   \n",
       "2                                            Shipped fast.   \n",
       "3                                               Five Stars   \n",
       "4                                               Five Stars   \n",
       "...                                                    ...   \n",
       "2047383                                         Excellent.   \n",
       "2047384                                         Five Stars   \n",
       "2047385                                         Four Stars   \n",
       "2047386                                         Five Stars   \n",
       "2047387                            Humorous and Insightful   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                       I love it and so does my students!  2015-08-31  \n",
       "1        My wife and I ordered 2 books and gave them as...  2015-08-31  \n",
       "2        Great book just like all the others in the ser...  2015-08-31  \n",
       "3                                             So beautiful  2015-08-31  \n",
       "4        Enjoyed the author's story and his quilts are ...  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2047383  Perfect print size and easy to handle. Too bad...  2015-02-24  \n",
       "2047384            Great series of books based in our area  2015-02-24  \n",
       "2047385  Amazing book- great story with incredible illu...  2015-02-24  \n",
       "2047386                                              Great  2015-02-24  \n",
       "2047387  It's fun read with both humor and insights int...  2015-02-24  \n",
       "\n",
       "[2047092 rows x 15 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "sample_predictions_list_final = []\n",
    "loaded_models_final = []\n",
    "for i in data_list:\n",
    "    X = i['review_body']\n",
    "    y = i['star_rating']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
    "    classifier =  LogisticRegression(multi_class='auto')\n",
    "    pipe = Pipeline([('cleaner', predictors()),\n",
    "                     ('tfidfVect', tfdifVect),\n",
    "                     ('classifier',classifier)])\n",
    "    pipe.fit(X_train,y_train)\n",
    "    filename = str(count) + 'finalized_model.sav'\n",
    "    pickle.dump(pipe, open(filename, 'wb'))\n",
    "    loaded_models_final.append(pickle.load(open(filename, 'rb')))\n",
    "    sample_predictions = loaded_models_final[count-1].predict(X_test)\n",
    "    sample_predictions_list_final.append(sample_predictions)\n",
    "    count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x3a9868490>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x3c73a4950>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x384011690>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x39328e490>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False), Pipeline(memory=None,\n",
       "          steps=[('cleaner', <__main__.predictors object at 0x33503b650>),\n",
       "                 ('vectorizer',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function spacy_tokenizer at 0x122bd6170>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('classifier',\n",
       "                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_models_final #All 5 Models for all 5 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 5, 5, ..., 5, 5, 3]),\n",
       " array([5, 5, 5, ..., 5, 5, 5]),\n",
       " array([5, 3, 5, ..., 4, 5, 5]),\n",
       " array([5, 5, 5, ..., 5, 5, 5]),\n",
       " array([5, 5, 5, ..., 5, 5, 1])]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predictions_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions_DataFrame = pd.DataFrame({'Batch1': sample_predictions_list_final[0],\n",
    "                                      'Batch2': sample_predictions_list_final[1],\n",
    "                                      'Batch3': sample_predictions_list_final[2],\n",
    "                                      'Batch4': sample_predictions_list_final[3],\n",
    "                                      'Batch5': sample_predictions_list_final[4]},index=Fals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch1</th>\n",
       "      <th>Batch2</th>\n",
       "      <th>Batch3</th>\n",
       "      <th>Batch4</th>\n",
       "      <th>Batch5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409414</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409415</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409416</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409417</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409418</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409419 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Batch1  Batch2  Batch3  Batch4  Batch5\n",
       "0            5       5       5       5       5\n",
       "1            5       5       3       5       5\n",
       "2            5       5       5       5       5\n",
       "3            5       5       5       5       4\n",
       "4            5       5       5       5       4\n",
       "...        ...     ...     ...     ...     ...\n",
       "409414       5       5       5       5       5\n",
       "409415       5       5       5       5       5\n",
       "409416       5       5       4       5       5\n",
       "409417       5       5       5       5       5\n",
       "409418       3       5       5       5       1\n",
       "\n",
       "[409419 rows x 5 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictions_DataFrame.to_csv('Predictions_DataFrame') # Ratings match the size of X_test\n",
    "Predictions_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 4, 1, 2]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = Predictions_DataFrame['Batch2'].unique().tolist() #confirmation for those batches which seem to \n",
    "                                                            #have l unique value only\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 4, 1]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = Predictions_DataFrame['Batch4'].unique().tolist() #confirmation for those batches which seem to \n",
    "                                                            #have l unique value only\n",
    "ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
